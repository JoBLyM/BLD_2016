#Data Engineering Assignment 1

##Assignment 1: Big Data in Ihrem Umfeld

###1.1 Schauen Sie sich in Ihrem Umfeld um. FH Technikum oder Ihr Job. Nennen Sie mindestens ein Beispiel für Daten, die schemalos (unstrukturiert) sind und mindestens ein Bespiel für Daten, die strukturiert (schematisch) sind.

Beispiele aus dem Berufsumfeld:
Unstrukturiert:
	* Kommunikation über diverse Nachrichtendienste (Emails, Skype, Messenger)
	* Schwarzes Brett

Strukturert:
	* Zeitaufzeichnungen
	* Urlaubsanträge

###1.2 Nennen Sie ein Beispiel für Daten in Ihrem Umfeld, die gestreamt verarbeitet werden, nennen Sie ein Beispiel für Daten in Ihrem Umfeld, die über Batchverarbeitung verarbeitet werden.

Beispiele aus dem Berufsumfeld:
Gestreamte Daten:
	* Übertragungen von Firmenpräsentationen

Batchdaten:
	* Tägliche Serverlogs
	
	

##Assignment 2: Big Data in Ihrem Umfeld

###Entscheiden Sie sich für eine Data Engineering Plattform. Apache Flink oder Apache Spark. Installieren Sie die auf Ihrem Arbeitsgerät.
###• Erklären Sie ihre Entscheidung

TEXT

###• Schicken Sie einen Screenshot der installierten Umgebung mit

TEXT

###• Beschreiben Sie Ihre Toolchain, die Sie mit dem Framework nutzen würden (z.B: IDE)

TEXT

##Assignment 3: Big Data in Ihrem Umfeld
###Schreiben Sie ein simples Program mit dem Framework (z.B. Helloworld) und laden Sie es hoch. 
###• 2 Punkte für Programm
###• 2 Punkte, wenn das Programm auch ausführbar ist.

#Data Science 
##Assignment 1: Technologien

###1.1 Sie haben in der LVA zwei Frameworks kennengelernt (R und Python). 
Nennen Sie zwei weitere Technologien, um Daten zu analysieren (müssen nicht open source sein)

Besonders bekannt ist hierbei Scala und Matlab, jedoch kann z.B. auch Mathematika für Datenanalyse verwendet werden.


###1.2 Sie bekommen den Auftrag, sich mit einer Data Science Technologie zu arbeiten. Nennen Sie Technologie, die ihnen auf dem ersten Blick am besten für Sie ersscheint und begründen Sie das!

Ich würde zu Python tendieren. Zwar bin ich geübt im Umgang mit R, jedoch bietet Python, neben seiner einfachen Syntax, eine Menge von Funktionen und ist mmn. für Programmierer ziemlich leicht zu erlernen. Die Community die hinter Python steht bietet auch eine Menge an Hilfestellung und Python selbst ist als Technologie bereits voll ausgereift.

##Assignment 2: Technologien
###Entscheiden Sie sich für eine Data Science Plattform. R oder Python Installieren Sie die auf Ihrem Arbeitsgerät.
###• Begründen Sie ihre Entscheidung (Warum ziehen Sie persönlich aus ihrer Ausgangssituation die eine Technologie der anderen vor).
TEXT

###• Schicken Sie einen Screenshot der installierten Umgebung mit
TEXT

###• Beschreiben Sie Ihre Toolchain, die Sie mit dem Framework nutzen (z.B. IDE)
TEXT

##Assignment 3: Big Science
###Der Cheatsheet auf http://scikit-learn.org/stable/tutorial/machine_learning_map/ ist eine einfache Anleitung, wie man den richtigen Algorithmus zum richtigen Data Science Problem findet.
###Schauen Sie in Google nach und lernen Sie classification, regression, clusting und dimensional reduction unterscheiden.
###Nennen Sie ein Beispiel aus ihrem Umfeld, wo Sie mit dem Algorithmus zu tun haben. Das kann ein Beispiel sein, wie: Wenn Sie bei Amazon einkaufen. Wenn Sie von einem Marketinginstitut angerufen werden, etc.

Classification: Daten werden versucht in Kategorien einzuordnen. Die Einordnung erfolgt mithilfe eines sogenannten "classifier" der mithilfe eines Trainingssampels trainiert wird. Ein Beispiel hierfür wären Emailfilter: in meinem Arbeitsumfeld werden so z.B. SVN-Commits, Arbeitsemails, potentielle Spam-Emails und Emails mit interessanten Inhalt (=Fortbildung/Ideenfindung, bei uns im Forschungsbereich werden solche Emails oft an den Teamverteiler weitergeleitet) eingeordnet.

Regression: Der Hauptunterschied zwischen Classification und Regression ist, dass Regression keine Kategorisierung vornimmt, sondern einen konkreten Wert vorraussagen kann. Dies wird vorallem bei Preisprognosen angewendet. 

Clustering: Clustering ist mehr oder weniger eine Erweiterung von Classification. Dabei wird Data nicht in vorher definierte Kategorien eingeteilt, sondern Kategorien werden im Zuge der Analyse erstellt. Ein Beispiel hierfür wäre die Amazon Produktvorschläge, wo mehrere Kategorien verbunden werden um ein perfektes Match zu kreieren.

Dimensonal Reduction: Dimensional Reduction wird bei großen Datenmengen verwendet um diese zu reduzieren, da ansonsten Analysen mit langen Bearbeitungszeiten rechnen müssen.